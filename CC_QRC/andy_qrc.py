import torch
import torch.nn.functional as f
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import namedtuple

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

batch = namedtuple(
    'batch',
    'states, nterm_sp, actions, rewards, gamma, term, nterm, size'
)

def getBatchColumns(samples):
    s, a, r, sp, gamma = list(zip(*samples))
    states = torch.cat(s)
    actions = torch.tensor(a, device=device).unsqueeze(1)
    rewards = torch.cat(r)
    gamma = torch.tensor(gamma, device=device)

    is_terminal = gamma == 0

    sps = [x for x in sp if x is not None]
    if len(sps) > 0:
        non_final_next_states = torch.cat(sps)
    else:
        non_final_next_states = torch.zeros((0, states.shape[1]))

    non_term = torch.logical_not(is_terminal).to(device)

    return batch(states, non_final_next_states, actions, rewards, gamma, is_terminal, non_term, len(samples))

def toNP(maybeTensor):
    if type(maybeTensor) == torch.Tensor:
        return maybeTensor.cpu()

    return maybeTensor

# much much faster than np.random.choice
def choice(arr, size=1):
    idxs = np.random.permutation(len(arr))
    return [arr[i] for i in idxs[:size]]

class ReplayBuffer:
    def __init__(self, buffer_size):
        self.buffer_size = buffer_size
        self.location = 0
        self.buffer = []

    def __len__(self):
        return len(self.buffer)

    def add(self, args):
        if len(self.buffer) < self.buffer_size:
            self.buffer.append(args)
        else:
            self.buffer[self.location] = args

        self.location = (self.location + 1) % self.buffer_size

    def sample(self, batch_size):
        return choice(self.buffer, batch_size), []

    # match api with prioritized ER buffer
    def update_priorities(self, idxes, priorities):
        pass


class Network(nn.Module):
    def __init__(self, input_size, h1_size, h2_size, output_size):
        super(Network, self).__init__()
        self.input_size = input_size
        self.h1_size = h1_size
        self.h2_size = h2_size
        self.output_size = output_size

        self.fc_1 = nn.Linear(self.input_size, self.h1_size, bias=True)
        self.fc_2 = nn.Linear(self.h1_size, self.h2_size, bias=True)
        self.fc_out = nn.Linear(self.h2_size, self.output_size, bias=True)

        self.init_weights()

    def init_weights(self):
        for layer in self.children():
            nn.init.xavier_uniform_(layer.weight.data)
            nn.init.normal_(layer.bias.data, 0, 0.1)

    def forward(self, x):
        x = f.relu(self.fc_1(x))
        features = f.relu(self.fc_2(x))

        # give back both the outputs of the Q network
        # and the "features" generated by the second to last layer
        return self.fc_out(features), features

    def cloneWeightsTo(self, toNet):
        toNet.load_state_dict(self.state_dict())


class BaseAgent:
    def __init__(self, features, actions, params):
        self.features = features
        self.actions = actions
        self.params = params

        # define parameter contract
        self.alpha = params['alpha']
        self.epsilon = params['epsilon']
        self.target_refresh = params['target_refresh']
        self.buffer_size = params['buffer_size']

        self.h1 = params['h1']
        self.h2 = params['h2']

        # build two networks, one for the "online" learning policy
        # the other as a fixed target network
        self.policy_net = Network(features, self.h1, self.h2, actions).to(device)
        self.target_net = Network(features, self.h1, self.h2, actions).to(device)

        # build the optimizer for _only_ the policy network
        # target network parameters will be copied from the policy net periodically
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.alpha, betas=(0.9, 0.999))

        # a simple circular replay buffer (i.e. a FIFO buffer)
        self.buffer = ReplayBuffer(self.buffer_size)
        self.steps = 0

        # initialize the weights of the target network to match the weights of policy network
        self.policy_net.cloneWeightsTo(self.target_net)

    def selectAction(self, x):
        # take a random action about epsilon percent of the time
        if np.random.rand() < self.epsilon:
            a = np.random.randint(self.actions)
            return torch.tensor(a, device=device)

        # otherwise take a greedy action
        q_s, _ = self.policy_net(x)
        # print(q_s)
        return q_s.argmax().detach()

    def updateNetwork(self, samples):
        pass

    def update(self, s, a, sp, r, gamma):
        # the "online" sample gets tossed into the replay buffer
        self.buffer.add((s, a, sp, r, gamma))
        self.steps += 1

        # if it is time to set the target net <- policy network
        # do that before the learning step
        if self.steps % self.target_refresh == 0:
            self.policy_net.cloneWeightsTo(self.target_net)

        # as long as we have enough samples in the buffer to do one mini-batch update
        # go ahead and randomly sample a mini-batch and do a single update
        if len(self.buffer) > 32:
            samples, idcs = self.buffer.sample(32)
            self.updateNetwork(samples)


class QRC(BaseAgent):
    def __init__(self, features, actions, params):
        super().__init__(features, actions, params)
        # regularization parameter
        self.beta = params['beta']

        # secondary weights optimization parameters
        self.beta_1 = params.get('beta_1', 0.99)
        self.beta_2 = params.get('beta_2', 0.999)
        self.eps = params.get('eps', 1e-8)

        # learnable parameters for secondary weights
        self.h = torch.zeros(self.actions, self.h2, requires_grad=False).to(device)
        # ADAM optimizer parameters for secondary weights
        self.v = torch.zeros(self.actions, self.h2, requires_grad=False).to(device)
        self.m = torch.zeros(self.actions, self.h2, requires_grad=False).to(device)

        self.updates = 0

    def updateNetwork(self, samples):
        self.updates += 1
        # organize the mini-batch so that we can request "columns" from the data
        # e.g. we can get all of the actions, or all of the states with a single call
        batch = getBatchColumns(samples)

        # compute Q(s, a) for each sample in mini-batch
        Qs, x = self.policy_net(batch.states)
        Qsa = Qs.gather(1, batch.actions).squeeze()

        # by default Q(s', a') = 0 unless the next states are non-terminal
        Qspap = torch.zeros(len(samples), device=device)

        # if we don't have any non-terminal next states, then no need to bootstrap
        if batch.nterm_sp.shape[0] > 0:
            Qsp, _ = self.target_net(batch.nterm_sp)

            # bootstrapping term is the max Q value for the next-state
            # only assign to indices where the next state is non-terminal
            Qspap[batch.nterm] = Qsp.max(1).values


        # compute the empirical MSBE for this mini-batch and let torch auto-diff to optimize
        # don't worry about detaching the bootstrapping term for semi-gradient Q-learning
        # the target network handles that
        target = batch.rewards + batch.gamma * Qspap.detach()
        td_loss = 0.5 * f.mse_loss(target, Qsa)

        # compute E[\delta | x] ~= <h, x>
        with torch.no_grad():
            delta_hats = torch.matmul(x, self.h.t())
            delta_hat = delta_hats.gather(1, batch.actions)

        # the gradient correction term is gamma * <h, x> * \nabla_w Q(s', a')
        # to compute this gradient, we use pytorch auto-diff
        correction_loss = torch.mean(batch.gamma * delta_hat * Qspap)

        # make sure we have no gradients left over from previous update
        self.optimizer.zero_grad()
        self.target_net.zero_grad()

        # compute the entire gradient of the network using only the td error
        td_loss.backward()

        # if we have non-terminal states in the mini-batch
        # the compute the correction term using the gradient of the *target network*
        if batch.nterm_sp.shape[0] > 0:
            correction_loss.backward()

        # add the gradients of the target network for the correction term to the gradients for the td error
        for (policy_param, target_param) in zip(self.policy_net.parameters(), self.target_net.parameters()):
            policy_param.grad.add_(target_param.grad)

        # update the *policy network* using the combined gradients
        self.optimizer.step()

        # update the secondary weights using a *fixed* feature representation generated by the policy network
        with torch.no_grad():
            delta = target - Qsa
            dh = (delta - delta_hat) * x

            # compute the update for each action independently
            # assume that there is a separate `h` vector for each individual action
            for a in range(self.actions):
                mask = (batch.actions == a).squeeze(1)

                # if this action was never taken in this mini-batch
                # then skip the update for this action
                if mask.sum() == 0:
                    continue

                # the update for `h` minus the regularizer
                h_update = dh[mask].mean(0) - self.beta * self.h[a]

                # ADAM optimizer with bias correction
                # keep a separate set of weights for each action here as well
                self.v[a] = self.beta_2 * self.v[a] + (1 - self.beta_2) * (h_update**2)
                self.m[a] = self.beta_1 * self.m[a] + (1 - self.beta_1) * h_update

                m = self.m[a] / (1 - self.beta_1**self.updates)
                v = self.v[a] / (1 - self.beta_2**self.updates)

                self.h[a] = self.h[a] + self.alpha * m / (torch.sqrt(v) + self.eps)
